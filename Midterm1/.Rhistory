t() %>%
data.frame()
test_prediction <- matrix(test_pred, nrow = 10,
ncol=135) %>%
t() %>%
data.frame() %>%
mutate(label = test_label + 1,
max_prob = max.col(., "last"))
length(test_label)
test_prediction <- matrix(test_pred, nrow = 10,
ncol=135) %>%
t() %>%
data.frame() %>%
mutate(label = test_label ,
max_prob = max.col(., "last"))
test_prediction <- matrix(test_pred, nrow = 10,
ncol=135) %>%
t() %>%
data.frame()
head(test_prediction)
test_prediction <- matrix(test_pred, nrow = 135,
ncol=10)
test_prediction <- matrix(test_pred, nrow = 135,
ncol=10) %>%
data.frame()
head(test_prediction)
length(test_label)
nrow(test_prediction)
test_pred <- predict(bst_model, newdata = test_matrix)
length(test_pred)
length(test_pred)/10
test_prediction <- matrix(test_pred, nrow = 1350,
ncol=10)
est_prediction <- matrix(test_pred, nrow = 1350,
ncol=10) %>%
data.frame() %>%
mutate(label = test_label ,
max_prob = max.col(., "last"))
dim(test_matrix)
test_pred <- predict(bst_model, newdata = test_matrix)
length(test_pred)
numberOfClasses <- 10
xgb_params <- list("objective" = "multi:softmax",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses+1)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label + 1)
head(OOF_prediction)
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
bst_model <- xgb.train(params = xgb_params,
data = train_matrix,
nrounds = nround)
test_pred <- predict(bst_model, newdata = test_matrix)
test_prediction <- matrix(test_pred, nrow = 1350,
ncol=10) %>%
data.frame() %>%
mutate(label = test_label ,
max_prob = max.col(., "last"))
length(test_pred)
test_pred
test_pred$label
# confusion matrix of test set
confusionMatrix(factor(test_pred),
factor(test_label),
mode = "everything")
head(test_matrix)
test_pred
names <-  colnames(wine.white[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = bst_model)
head(importance_matrix)
summary(test_data)
summary(train_data)
data_variables <- as.matrix(wine.white[,-1])
data_variables
wine.white
names(wine.white)
data_variables <- as.matrix(wine.white[,-1])
data_variables
wine.white[,-1]
class(wine.white)
wine.white[1]
wine.white[-1]
dim(wine.white)
wine.white[,"quality"]
wine.white[,-"quality"]
wine.white[,1]
wine.white[,11]
wine.white[,12]
wine.white[,-12]
# Make split index
train_index <- sample(1:nrow(wine.white), nrow(wine.white)*0.75)
# Full data set
data_variables <- as.matrix(wine.white[,-12])
data_label <- wine.white[,"quality"]
data_matrix <- xgb.DMatrix(data = as.matrix(wine.white), label = data_label)
# split train data and make xgb.DMatrix
train_data   <- data_variables[train_index,]
train_label  <- data_label[train_index]
train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
# split test data and make xgb.DMatrix
test_data  <- data_variables[-train_index,]
test_label <- data_label[-train_index]
test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
numberOfClasses <- 10
xgb_params <- list("objective" = "multi:softmax",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses+1)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
library(dplyr)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label + 1)
head(OOF_prediction)
# confusion matrix
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
bst_model <- xgb.train(params = xgb_params,
data = train_matrix,
nrounds = nround)
# Predict hold-out test set
test_pred <- predict(bst_model, newdata = test_matrix)
# confusion matrix of test set
confusionMatrix(factor(test_pred),
factor(test_label),
mode = "everything")
data_matrix <- xgb.DMatrix(data = as.matrix(wine.white), label = data_label)
train_data   <- data_variables[train_index,]
train_label  <- data_label[train_index]
train_label
train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
test_data  <- data_variables[-train_index,]
test_label <- data_label[-train_index]
test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
numberOfClasses <- 10
xgb_params <- list("objective" = "multi:softmax",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses+1)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
library(dplyr)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label + 1)
head(OOF_prediction)
numberOfClasses <- 10
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses+1)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label + 1)
head(OOF_prediction)
# confusion matrix
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
numberOfClasses <- 10
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
library(dplyr)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label + 1)
head(OOF_prediction)
# confusion matrix
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
length(cv$pred)
length(cv_model$pred)
length(train_label)
library(dplyr)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label )
head(OOF_prediction)
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
summary(OOF_prediction$label)
summary(OOF_prediction$max_prob)
ww <- wine.white %>% mutate(quality = quality -1)
train_index <- sample(1:nrow(ww), nrow(ww)*0.75)
data_variables <- as.matrix(ww[,-12])
data_label <- wine.white[,"quality"]
data_matrix <- xgb.DMatrix(data = as.matrix(ww), label = data_label)
train_data   <- data_variables[train_index,]
train_label  <- data_label[train_index]
train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
test_data  <- data_variables[-train_index,]
test_label <- data_label[-train_index]
test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
numberOfClasses <- 10
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label + 1)
head(OOF_prediction)
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob+1),
mode = "everything")
numberOfClasses <- 10
xgb_params <- list("objective" = "multi:softmax",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
numberOfClasses <- 9
xgb_params <- list("objective" = "multi:softmax",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label )
head(OOF_prediction)
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob+1),
mode = "everything")
numberOfClasses <- 9
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label )
head(OOF_prediction)
# confusion matrix
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob+1),
mode = "everything")
ww <- wine.white %>% mutate(quality = quality -1)
# Make split index
train_index <- sample(1:nrow(ww), nrow(ww)*0.75)
# Full data set
data_variables <- as.matrix(ww[,-12])
# Make split index
train_index <- sample(1:nrow(ww), nrow(ww)*0.75)
# Full data set
data_variables <- as.matrix(ww[,-12])
data_label <- ww[,"quality"]
data_matrix <- xgb.DMatrix(data = as.matrix(ww), label = data_label)
# split train data and make xgb.DMatrix
train_data   <- data_variables[train_index,]
train_label  <- data_label[train_index]
train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
# split test data and make xgb.DMatrix
test_data  <- data_variables[-train_index,]
test_label <- data_label[-train_index]
test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
numberOfClasses <- 9
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label )
head(OOF_prediction)
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
bst_model <- xgb.train(params = xgb_params,
data = train_matrix,
nrounds = nround)
test_pred <- predict(bst_model, newdata = test_matrix)
confusionMatrix(factor(test_pred),
factor(test_label),
mode = "everything")
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = train_label+1 )
confusionMatrix(factor(OOF_prediction$label),
factor(OOF_prediction$max_prob),
mode = "everything")
bst_model <- xgb.train(params = xgb_params,
data = train_matrix,
nrounds = nround)
# Predict hold-out test set
test_pred <- predict(bst_model, newdata = test_matrix)
test_prediction <- matrix(test_pred, nrow = numberOfClasses,
ncol=length(test_pred)/numberOfClasses) %>%
t() %>%
data.frame() %>%
mutate(label = test_label + 1,
max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(test_prediction$label),
factor(test_prediction$max_prob),
mode = "everything")
# get the feature real names
names <-  colnames(dat[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = bst_model)
head(importance_matrix)
names <-  colnames(ww)
importance_matrix = xgb.importance(feature_names = names, model = bst_model)
head(importance_matrix)
err <- mean(test_prediction$label != test_prediction$max_prob)
print(paste("test-error=", err))
gp = xgb.plot.importance(importance_matrix)
print(gp)
model <- randomForest(quality ~ . , data = train_data)
library(randomForest)
install.packages("randomForest")
train_data   <- ww[train_index,-12]
model <- randomForest(quality ~ . , data = train_data)
library(randomForest)
model <- randomForest(quality ~ . , data = train_data)
train_data
head(train_data)
train_data   <- ww[train_index]
model <- randomForest(quality ~ . , data = train_data)
train_data   <- ww[train_index,]
model <- randomForest(quality ~ . , data = train_data)
pred <- predict(model, newdata = test_data)
pred
train_data$quality <- as.factor(train_data$quality)
model <- randomForest(quality ~ . , data = train_data)
test_data <- ww[-train_index,]
test_data$quality <- as.factor(test_data)
test_data$quality <- as.factor(test_data$quality)
pred <- predict(model, newdata = test_data)
pred
err <- mean(pred != test_data$quality)
pred
length(pred)
length(test_data$quality)
pred == test_data$quality
train_data   <- ww[train_index,]
test_data <- ww[-train_index,]
#train_data$quality <- as.factor(train_data$quality)
model <- randomForest(quality ~ . , data = train_data)
#test_data$quality <- as.factor(test_data$quality)
pred <- predict(model, newdata = test_data)
err <- mean(round(pred,1) != test_data$quality)
err
pred
err <- mean(round(pred,0) != test_data$quality)
err
1-err
library(glmnet)
install.packages("glmnet")
library(glmnet)
library(leaps)
install.packages("leaps")
install.packages("MASS")
install.packages("MASS")
library(glmnet)
library(leaps)
library(ggplot2)
library(MASS)
knitr::opts_chunk$set(echo = TRUE)
install.packages("knitr")
library(glmnet)
library(leaps)
library(ggplot2)
library(MASS)
knitr::opts_chunk$set(echo = TRUE)
wine.white <- read.csv("winequality-white.csv", header = TRUE, sep=";")
wine.red <- read.csv("winequality-red.csv", header = TRUE, sep=";")
summary(wine.white)
par(mfrow=c(1,1))
plot(fixed.acidity)
attach(wine.white)
#We can see one/two major outliers that we can remove
par(mfrow=c(1,1))
plot(fixed.acidity)
boxplot(fixed.acidity)
dev.off
dev.off()
plot(fixed.acidity)
plot(volatile.acidity)
plot(cars)
plot(fixed.acidity)
fixed.acidity
plot(fixed.acidity)
plot(fixed.acidity)
plot(volatile.acidity)
plot(fixed.acidity)
plotType(domain = wine.white, FUN=boxplot)
plot(residual.sugar)
summary(wine.white)
dim(wine.white)
head(wine.white)
plotType <- function(domain=wine.white,FUN=hist){
FUN = match.fun(FUN)
par(mfrow=c(3,4))
for (i in 1:dim(domain)[2]){
FUN(domain[,i],xlab = names(domain)[i],main = "")
}
}
plotType(domain = wine.white, FUN=plot)
plotType(domain = wine.white, FUN=hist)
plotType(domain = wine.white, FUN=plot)
plotType(domain = wine.white, FUN=boxplot)
hist(log(residual.sugar))
hist(log(volatile.acidity))
plotType(domain = wine.white, FUN=plot)
plot(citric.acid[1400:1700])
plot(citric.acid[1400:1700])
plotType(domain = wine.white, FUN=hist)
log.wine.white <- log(wine.white)
plotType(domain = log.wine.white, FUN=hist)
plotType(domain = log.wine.white, FUN=plot)
plotType(domain = wine.white, FUN=plot)
plotType(domain = log.wine.white, FUN=hist)
plotType(domain = wine.white, FUN=boxplot)
plotType(domain = log.wine.white, FUN=boxplot)
plotType(domain = log.wine.white, FUN=boxplot)
plotType(domain = log.wine.white, FUN=boxplot)
plotType <- function(domain=wine.white,FUN=hist){
FUN = match.fun(FUN)
par(mfrow=c(4,4))
for (i in 1:dim(domain)[2]){
FUN(domain[,i],xlab = names(domain)[i],main = "")
}
}
plotType(domain = log.wine.white, FUN=boxplot)
par("mar")
